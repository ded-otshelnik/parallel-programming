# Задача 6. Параллельная технология MPI

## Содержание

## Технология MPI: классификация Флинна, модель параллелизма

Как говорилось в [теме про OpenMP](4rd-task.md#классификация-флинна-и-модель-параллелизма-mimd) системы MIMD делятся на два типа по типу организации оперативной памяти: **мультипроцессоры (с общей разделяемой памятью)** и **мультикомпьютеры (системы с разделяемой памятью)**. Если технология OpenMP относится к мультипроцессорам, то для параллелизма на мультикомпьютерах используется технология **MPI**.

Согласно официальному описанию, **MPI (Message-Passing Interface)** - это спецификация API библиотек для передачи данных. Стандарт MPI образуют процедуры передачи сообщений, а также процедуры ввода/вывода, использующие ресурсы параллелизма. Сама спецификация выпущена в 1994 году, на текущий момент актуальной версией спецификации является **MPI-4.0**, но далее будет описываться спецификация **[MPI-3.0](https://www.mpi-forum.org/docs/mpi-3.0/mpi30-report.pdf)**.

В рамках технологии MPI единая программа, написанная на высокоуровневом языке программирования (C/C++, Fortran, иногда даже Python), запускается на процессорах вычислительной системы, образуя, тем самым, **множество процессов**. Т.е. программа на MPI (в стадии выполнения) - множество параллельных взаимодействующих процессов. Каждый процесс обладает своим уникальным порядковым номером - **рангом**, что позволяет явно управлять действиями на различных вычислительных узлах. Обмен данными, требуемый для функционирования программы, осуществляется через механизм передачи сообщений путем вызова библиотечных функций, использующих возможности коммуникационных линий связи.

## Компиляция и запуск параллельной программы в среде MPI. Общая структура кода

Параллельная программа, использующая технологию MPI, должна быть скомпилирована и запущена особым образом, определяемым **реализацией стандарта**. Широко распространены две реализации: **OpenMPI** и **MPICH**. Далее будет рассмотрена реализация [OpenMPI](https://docs.open-mpi.org/en/v5.0.x/index.html):

+ Код параллельной программы представляется в виде файла с расширением ```.c``` и должен содержать в секции подключение заголовочных файлов ```#include <mpi.h>```.
+ Для запуска программы ***task.c*** требуется компилировать с помощью компилятора **mpicc** следующим образом:

```bash
mpicc task.c -o <output_name>
```

+ Запуск бинарного файла, полученного при компиляции, с **образованием** $N$ **процессов** осуществляется через команду **mpirun** следующим образом:

```bash
mpirun -n N <binary_file>
```

Инструкция как запускать параллельной программы в среде MPI на суперкомпьютере ВГУ представлена [здесь](supercomputer-guide.md).

Теперь рассмотрим важнейшие функции среды MPI, встречающиеся в **любой** параллельной программе. Сразу после объявления переменных в функции ***main*** должен следовать вызов функции ```MPI_Init()```, который инициализирует параллельную часть программы:

```c
int MPI_Init(int* argc, char*** argv)
```

где **argc** и **argv** - указатели на количество параметров командной строки и на массив указателей на строки соответственно. Параллельная часть программы завершается вызовом функции ```MPI_Finalize()```:

```c
int MPI_Finalize(void)
```

Вне параллельной области программы вызов функций MPI запрещен, кроме следующих четырех функций:

```c
int MPI_Get_version(int* version, int* subversion);
int MPI_Get_library_version(char* version, int* resultlen);
int MPI_Initialized(int* flag);
int MPI_Finalized(int* flag);
```

Важным понятием технологии является **коммуникатор**, под которым понимают группу процессов, которому поставлен в соответствие **дескриптор** (специальное описание объектов). Сразу после вызова функции *MPI_Init* формируется и становится доступным **глобальный коммуникатор** ```MPI_COMM_WORLD```. Такой коммуникатор содержит все доступные процессы.

Функция ```MPI_Comm_size()``` определяет число процессов, входящий в некоторый коммуникатор *comm*.

```c
int MPI_Comm_size(MPI_Comm comm, int* size);
```

В частности, вызов

```c
MPI_Comm_size(MPI_COMM_WORLD, &size);
```

сформирует число процессов, исполняющих программу, и запишет в переменную.

Для определения ранга процесса используется функция ```MPI_Comm_rank()```

```c
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
```

Если в результате работы приложения происходит какая-то ошибка или требуется по какой-то иной причине завершить программу принудительно, то для этого используется функция ```MPI_Abort()```, которая завершает все процессы, ассоциированные с коммуникатором *comm* корректным образом:

```c
int MPI_Abort(MPI_Comm comm, int errorcode);
```

## Стандартный способ передачи сообщений

Под **сообщением** понимается вектор данных фиксированного типа. Некоторые доступные типы представлены [здесь](https://rookiehpc.org/mpi/docs/mpi_datatype/index.html).

Рассмотрим простейший способ передачи сообщения - **point-to-point communication (попарный обмен сообщениями между процессами)**. Для попарного обмена сообщениями используются две функции ```MPI_Send()``` и ```MPI_Recv()```:

```c
int MPI_Send(const void* buf, int count, MPI_Datatype datatype,
             int dest, int tag, MPI_Comm comm)
int MPI_Recv(void* buf, int count, MPI_Datatype datatype,
             int source, int tag, MPI_Comm comm, MPI_Status* status)
```

где *buf* - адрес буфера памяти с пересылаемым сообщением, *count* - длина сообщения, *datatype* - тип данных, *source* - ранг процесса-отправителя, *dest* - ранг процесса-получателя, *tag* - идентификатор сообщения, *comm* - коммуникатор, содержащий оба процесса, *status* - указатель на структуру, содержащую некоторую информацио о принятом сообщении.

Несмотря на свою простоту, неправильный вызов функций может привести к "тупику" (**deadlock**), когда оба процесса заблокированы из-за невозможности выполнить обмен сообщениями. Для преодоления подобной проблемы существует функция ```MPI_Sendrecv()```, которая объединяет действия двух функций.

```c
int MPI_Sendrecv(const void* sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag,
                 void* recvbuf, int recvcount, MPI_Datatype recvtype, int source, int resvtag,
                 MPI_Comm comm, MPI_Status* status)
```

По умолчанию адреса *sendbuf* и *recvbuf* не должны совпадать. Если требуется изменить данные внутри одного буфера, то для этого используется функция ```MPI_Sendrecv_replace()```:

```c
int MPI_Sendrecv_replace(void* buf, int count, MPI_Datatype datatype,
                 int dest, int sendtag,
                 int source, int resvtag,
                 MPI_Comm comm, MPI_Status* status)
```

Если требуется, чтобы процесс получал от любого источника, отправлял любому получателю или принимал сообщения с любым тегом, то для этого предусмотрены параметры **MPI_ANY_SOURCE**, **MPI_ANY_DEST** и **MPI_ANY_TAG**.

## Измерение времени

Для оценки производительности параллельных программ на MPI cуществуют функции ```MPI_Wtick()``` и ```MPI_Wtime()```, измеряющие время, причем первая функция измеряет время относительно некоторого момента в прошлом, вторая - относительно разницы между двумя последовательными отсчетами времени в рамках системного таймера.

## Способы передачи сообщений

Помимо стандартного способа передачи сообщений, который предполагает наличие системного буфера для чтения/записи сообщений, что не всегда возможно и при его отсутствии может произойти блокировка процесса, существуют еще несколько способов отправки данных:

+ **Буферизированный способ (buffered)**: способ позволяет завершить передачу сообщения на стороне процесса-отправителя еще до того, как будет вызвана принимающая функция. В программе должен быть выделен пользовательский буфер, размер которого должен быть достаточным для работы с передаваемыми данными.
+ **Синхронный способ (synchronous)**: в данном способе операция процесса-отправителя может быть завершена, только если процесс-получатель сообщил о начале приема данного сообщения, что может приводить к временным задержкам, но при этом избегая большого числа буферизированных сообщений.
+ **Способ "По готовности" (ready)**: способ характеризируется тем, что операция передачи сообщения может быть начата только при условии инициализации получающей процедуры и готовности ее к приему, но при этом результат невыполнения условия не определен.

Кроме того, каждый из перечисленных методов (включая стандартный) доступен в двух вариантах - **блокирующем** и **неблокирующем**.

Вид способа связи учитывается в названии функции, параметры совпадают с параметрами функции ```MPI_Send()```:

+ ```MPI_Bsend()``` - буферизированный способ
+ ```MPI_Ssend()``` - синхронный способ
+ ```MPI_Rsend()``` - способ "по готовности"

Использования буферизированного способа подразумевает выделение памяти через следующую функцию:

```c
int MPI_Buffer_attach(void* buffer, int size)
```

и освобождение в конце:

```c
int MPI_Buffer_attach(void* buffer_addr, int* size)
```

Неблокирующие функции обычно содержат префикс "I": ```MPI_Isend()```, ```MPI_Ibsend()```, ```MPI_Issend()```, ```MPI_Irsend()```. Неблокирующая операция приема предусмотрена всего одна - ```MPI_Irecv()```.

Для идентификации неблокирующих передач данных используются служебные структуры предопределенного типа **MPI_Request**, которые указываются как параметры при вызове неблокирующих функций.

Обычно заранее неизвестно когда закончится неблокирующая операция. Для ожидания завершения используются специальные функции, которые блокируют процесс до завершения операций, определенных через *request*:

```c
int MPI_Wait(MPI_Request* request, MPI_Status* status);
int MPI_Waitall(int count, MPI_Request requests[], MPI_Status statuses[]);
```

Для проверки выполнения некоторой операции, определенной через *request*, используются следующие функции, где *flag* - параметр завершения операции (при $\neq$ 0 проверяется завершение, при 0 - их незавершение):

```c
int MPI_Test(MPI_Request* request, int* flag, MPI_Status* status);
int MPI_Testall(int count, MPI_Request requests[], int* flag, MPI_Status statuses[]);
```

## Коллективные взаимодействия

**Коллективные взаимодействия** обеспечивают обмен данными между всеми процессами, принадлежащими некоторому коммуникатору. Технологией MPI предусмотрены следующие коллективные операции:

+ синхронизация
+ 

## Задания

## Требования

## Варианты

1. f
2. f
3. f
4. f
5. f
6. f
7. f
8. f
9. f
10. f
11. f
12. f
13. f
14. f
